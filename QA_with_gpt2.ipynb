{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vilmo18/Languages_Models_Without_Entity_Knowledge/blob/main/QA_with_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "uwkLI6KU4HOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI5n0L5wtIGQ",
        "outputId": "a8412167-012f-48bd-9e09-f8bead9576f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets\n",
        "!pip install --upgrade pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk4P30JJudng",
        "outputId": "8daa1ff1-2c75-471f-f9e0-5415e59e4e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O3otI3M45qc",
        "outputId": "fd8510f2-ecc7-46c2-bebe-e37ca6e668ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=3fe09d08fcf04713ed33cf2134733cfa984b844f551176517dfbdbf3045c18d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA with entities"
      ],
      "metadata": {
        "id": "SIBzrbIh4PnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset, Dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "cfffnGNRw9Aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d13c60c-56ed-437a-bca3-9cff887ec266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
        "anonymized_subset=load_from_disk(\"qa_good\")\n",
        "anonymized_subset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wGjvVZKC1s7",
        "outputId": "b549b626-205c-4074-b222-bdaddef76d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id=14\n",
        "anonymized_subset['train'][id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJA62ivEj_pB",
        "outputId": "9ce9e4f1-20b8-47cd-b138-be843f8a4a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733bed24776f4190066118c',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'The university is the major seat of ent113595 (albeit not its official headquarters, which are in ent185714). Its main seminary, ent21422, is located on the campus across St. ent148252 lake from ent211845. ent242011, the oldest building on campus and located near the shore of St. ent220024, houses undergraduate seminarians. Retired priests and brothers reside in ent10640 (a former retreat center), ent173298, as well as ent119769 near the Grotto. The university through ent63023 has ties to theologian ent144130. While not ent44296, ent46377 has praised writers from ent299290 and ent21422 created ent69726.',\n",
              " 'question': 'Which prize did ent144130 create?',\n",
              " 'answers': {'answer_start': [601], 'text': ['ent72089']}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id=14\n",
        "dataset['train'][id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmCcf9z97DlQ",
        "outputId": "1f0ca34d-54ac-4ab3-fc49-92b4f5e8743a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733bed24776f4190066118c',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.',\n",
              " 'question': 'Which prize did Frederick Buechner create?',\n",
              " 'answers': {'text': ['Buechner Prize for Preaching'], 'answer_start': [675]}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id=1\n",
        "anonymized_subset['train'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXx8U68I6KeX",
        "outputId": "10b8748b-66af-4f9c-f8b3-46bc614de73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661180',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a ent93676 character. Atop ent289216 gold dome is a golden statue of ent68466. Immediately in front of ent285133 and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to ent285133 is ent182288. Immediately behind the basilica is the Grotto, a ent78755 place of prayer and reflection. It is a replica of the grotto at ent249113, ent154860 where ent68466 reputedly appeared to ent223217 in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and ent209005), is a simple, modern stone statue of ent86434.',\n",
              " 'question': 'ent21984 at ent59357 is beside to which structure?',\n",
              " 'answers': {'answer_start': [254], 'text': ['ent285133']}}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsrm9cwZkUA6",
        "outputId": "e9ae5391-995e-4d18-f41f-ac391b65a3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56d3726559d6e414001463e9',\n",
              " 'title': 'American_Idol',\n",
              " 'context': 'Corey Clark was disqualified during the finals for having an undisclosed police record; however, he later alleged that he and Paula Abdul had an affair while on the show and that this contributed to his expulsion. Clark also claimed that Abdul gave him preferential treatment on the show due to their affair. The allegations were dismissed by Fox after an independent investigation. Two semi-finalists were also disqualified that year – Jaered Andrews for an arrest on an assault charge, and Frenchie Davis for having previously modelled for an adult website.',\n",
              " 'question': 'Which contestant was removed from the competition for not revealing his police record?',\n",
              " 'answers': {'text': ['Corey Clark'], 'answer_start': [0]}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ids_dataset1 = set(anonymized_subset['train']['id'])\n",
        "validation_ids_dataset1 = set(anonymized_subset['validation']['id'])\n",
        "\n",
        "# Filtrer le dataset2 pour ne garder que les éléments qui sont aussi dans dataset1\n",
        "filtered_train_dataset2 = dataset['train'].filter(lambda example: example['id'] in train_ids_dataset1)\n",
        "filtered_validation_dataset2 = dataset['validation'].filter(lambda example: example['id'] in validation_ids_dataset1)\n",
        "\n",
        "# Créer le nouveau DatasetDict filtré\n",
        "dataset = DatasetDict({\n",
        "    'train': filtered_train_dataset2,\n",
        "    'validation': filtered_validation_dataset2\n",
        "})\n",
        "\n",
        "# Afficher le résultat\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpXvkVZ_Cv9q",
        "outputId": "51322557-fdf3-443c-b72a-565ae2245463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 75608\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 8663\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "pwCfenQXdTwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13780f2-a17e-416c-d34b-c30453f7c69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset['train'][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVVPGcOKG3Sm",
        "outputId": "bc9baa88-bd83-49ea-912a-cea02b9c26d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661181',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'What is the Grotto at Notre Dame?',\n",
              " 'answers': {'text': ['a Marian place of prayer and reflection'],\n",
              "  'answer_start': [381]}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        # stride=128,\n",
        "        # return_overflowing_tokens=True,\n",
        "        # #return_offsets_mapping=True,\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] != 1: # Check if idx is within bounds\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while idx < len(sequence_ids) and sequence_ids[idx] == 1: # Check if idx is within bounds\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "AORHuPh8MYHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "GK1lPd6Yv3H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from transformers import GPT2Tokenizer, GPT2ForQuestionAnswering, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "h7ca1yFPd2cN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb03396-64ab-46e6-e710-70ff8019dfdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ6qhEG4X_Jt",
        "outputId": "3e890971-79d6-4767-edd6-f16f90718029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader  #\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq,DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "#model.config.use_cache = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    warmup_steps=1000,\n",
        "    gradient_accumulation_steps=2,\n",
        "    max_steps=5000,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=8,  # 24 - 8\n",
        "    weight_decay=0.01,\n",
        "    #dataloader_num_workers=4,\n",
        "    save_steps=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    fp16=True,\n",
        "    #report_to=\"tensorboard\",\n",
        "    eval_steps=0.1,\n",
        "    seed=4201,\n",
        "    #remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJkLSuO0ti9J",
        "outputId": "0129751d-99ff-45b8-8f6e-09c0800afb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import Trainer, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader  #\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq,DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "#model.config.use_cache = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    warmup_steps=500,\n",
        "    #gradient_accumulation_steps=2,\n",
        "    max_steps=10000,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=4,  # 24 - 8\n",
        "    weight_decay=0.01,\n",
        "    dataloader_num_workers=4,\n",
        "    save_steps=0.1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    fp16=True,\n",
        "    #report_to=\"tensorboard\",\n",
        "    eval_steps=0.1,\n",
        "    seed=42,\n",
        "    #remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOxJ8i3FyZ7i",
        "outputId": "d0466d77-1d9f-4bfb-f4d6-913dbd68a0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "tI20257qdqN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b0b5af2-0292-4d40-9b4a-4933ad22e71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarre\u001b[0m (\u001b[33mcarre-su\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.18.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241015_071223-mu32jlkx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/carre-su/huggingface/runs/mu32jlkx' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/carre-su/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/carre-su/huggingface' target=\"_blank\">https://wandb.ai/carre-su/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/carre-su/huggingface/runs/mu32jlkx' target=\"_blank\">https://wandb.ai/carre-su/huggingface/runs/mu32jlkx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 4:06:46, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.347300</td>\n",
              "      <td>2.134022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.009200</td>\n",
              "      <td>1.842003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.730900</td>\n",
              "      <td>1.760921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.736800</td>\n",
              "      <td>1.640930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.707900</td>\n",
              "      <td>1.608805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.556900</td>\n",
              "      <td>1.543980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.564900</td>\n",
              "      <td>1.481084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.473900</td>\n",
              "      <td>1.428870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.429900</td>\n",
              "      <td>1.387852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.420100</td>\n",
              "      <td>1.366873</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10000, training_loss=1.7899933044433594, metrics={'train_runtime': 14816.3841, 'train_samples_per_second': 5.399, 'train_steps_per_second': 0.675, 'total_flos': 2.090374053888e+16, 'train_loss': 1.7899933044433594, 'epoch': 0.91324200913242})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "-jRFFr9weJBA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "d8b84b27-278a-4ae2-aca5-6ee734690ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='661' max='661' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [661/661 03:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.3668725490570068,\n",
              " 'eval_runtime': 192.4775,\n",
              " 'eval_samples_per_second': 54.916,\n",
              " 'eval_steps_per_second': 3.434,\n",
              " 'epoch': 0.91324200913242}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "ACV23f9u4u7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "\n",
        "def generate_answer(question, context, model=model, tokenizer=tokenizer, top_k=1):\n",
        "    # Encode inputs\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get model outputs\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Compute span scores\n",
        "    start_scores = outputs.start_logits[0].cpu().numpy()\n",
        "    end_scores = outputs.end_logits[0].cpu().numpy()\n",
        "\n",
        "    # Get top-k start and end positions\n",
        "    start_indices = start_scores.argsort()[-top_k:][::-1]\n",
        "    end_indices = end_scores.argsort()[-top_k:][::-1]\n",
        "\n",
        "\n",
        "    answers = []\n",
        "    for start_index in start_indices:\n",
        "        for end_index in end_indices:\n",
        "            if end_index >= start_index:\n",
        "                answer_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_index:end_index + 1])\n",
        "                answer = tokenizer.convert_tokens_to_string(answer_tokens).strip()\n",
        "                if answer:\n",
        "                    answers.append((answer, start_scores[start_index] + end_scores[end_index]))\n",
        "\n",
        "    answers = sorted(answers, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return answers[0][0] if answers else \"No answer found\"\n",
        "\n",
        "\n",
        "\n",
        "context = \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance arch to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\"\n",
        "\n",
        "questions = [\n",
        "    \"Who designed the Eiffel Tower?\",\n",
        "    \"When was the Eiffel Tower built?\",\n",
        "    \"Where is the Eiffel Tower located?\",\n",
        "    \"What was the initial reaction to the Eiffel Tower?\",\n",
        "    \"What is the Eiffel Tower made of?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = generate_answer(question, context,model,tokenizer)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "PCInWl6RCJY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c694d74-d755-46e2-aa0f-6ea3dabf5de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who designed the Eiffel Tower?\n",
            "Answer: Gustave Eiffel\n",
            "\n",
            "Question: When was the Eiffel Tower built?\n",
            "Answer: 1887 to 1889\n",
            "\n",
            "Question: Where is the Eiffel Tower located?\n",
            "Answer: Paris, France\n",
            "\n",
            "Question: What was the initial reaction to the Eiffel Tower?\n",
            "Answer: criticized\n",
            "\n",
            "Question: What is the Eiffel Tower made of?\n",
            "Answer: a wrought-iron lattice tower\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx=854\n",
        "test_question = dataset['validation'][idx]['question']\n",
        "test_context = dataset['validation'][idx]['context']\n",
        "generated_answer = generate_answer(test_question, test_context,model,tokenizer)\n",
        "print(f\"Generated answer: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "JNlZV7sxLFHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f20f07-dd30-4fa2-bc87-b1c2b0389ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated answer: Ogród Saski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MfX58gBJDo3",
        "outputId": "22c52a3a-3468-4e4b-c2a2-4ef112ec502f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56dfa2c54a1a83140091ebf2',\n",
              " 'title': 'Nikola_Tesla',\n",
              " 'context': \"In 1873, Tesla returned to his birthtown, Smiljan. Shortly after he arrived, Tesla contracted cholera; he was bedridden for nine months and was near death multiple times. Tesla's father, in a moment of despair, promised to send him to the best engineering school if he recovered from the illness (his father had originally wanted him to enter the priesthood).\",\n",
              " 'question': 'What disease did Tesla contract in 1873?',\n",
              " 'answers': {'text': ['cholera', 'cholera', 'cholera'],\n",
              "  'answer_start': [94, 94, 94]}}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "9TBs0o_v5Gug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./results\n"
      ],
      "metadata": {
        "id": "MPOHw1S-LMSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "def evaluate_model_with_rouge(model, tokenizer, dataset):\n",
        "    exact_match_total = 0\n",
        "    f1_total = 0\n",
        "    rouge_1_total = 0\n",
        "    rouge_2_total = 0\n",
        "    rouge_l_total = 0\n",
        "\n",
        "    num_samples = len(dataset['validation'])\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    for i in tqdm(range(num_samples)):\n",
        "        question = dataset['validation'][i]['question']\n",
        "        context = dataset['validation'][i]['context']\n",
        "        ground_truths = dataset['validation'][i]['answers']['text']\n",
        "\n",
        "        generated_answer = generate_answer(question, context)\n",
        "\n",
        "        exact_match_scores = [int(generated_answer.strip() == truth.strip()) for truth in ground_truths]\n",
        "        exact_match_total += max(exact_match_scores)\n",
        "\n",
        "        f1_scores = []\n",
        "        rouge_1_scores = []\n",
        "        rouge_2_scores = []\n",
        "        rouge_l_scores = []\n",
        "\n",
        "        for truth in ground_truths:\n",
        "            generated_tokens = generated_answer.split()\n",
        "            ground_truth_tokens = truth.split()\n",
        "\n",
        "            common_tokens = set(generated_tokens) & set(ground_truth_tokens)\n",
        "            precision = len(common_tokens) / len(generated_tokens) if generated_tokens else 0\n",
        "            recall = len(common_tokens) / len(ground_truth_tokens) if ground_truth_tokens else 0\n",
        "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "            rouge_scores = scorer.score(generated_answer, truth)\n",
        "            rouge_1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "            rouge_2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
        "            rouge_l_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "        f1_total += max(f1_scores)\n",
        "        rouge_1_total += max(rouge_1_scores)\n",
        "        rouge_2_total += max(rouge_2_scores)\n",
        "        rouge_l_total += max(rouge_l_scores)\n",
        "\n",
        "    avg_exact_match = exact_match_total / num_samples\n",
        "    avg_f1_score = f1_total / num_samples\n",
        "    avg_rouge_1 = rouge_1_total / num_samples\n",
        "    avg_rouge_2 = rouge_2_total / num_samples\n",
        "    avg_rouge_l = rouge_l_total / num_samples\n",
        "\n",
        "    return avg_exact_match, avg_f1_score, avg_rouge_1, avg_rouge_2, avg_rouge_l"
      ],
      "metadata": {
        "id": "NtB2VZquAyGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_exact_match, avg_f1_score, avg_rouge_1, avg_rouge_2, avg_rouge_l = evaluate_model_with_rouge(model, tokenizer, dataset)\n",
        "print(f\"Exact Match: {avg_exact_match:.2f}\")\n",
        "print(f\"F1 Score: {avg_f1_score:.2f}\")\n",
        "print(f\"ROUGE-1: {avg_rouge_1:.2f}\")\n",
        "print(f\"ROUGE-2: {avg_rouge_2:.2f}\")\n",
        "print(f\"ROUGE-L: {avg_rouge_l:.2f}\")"
      ],
      "metadata": {
        "id": "IKSdOSvsA1uS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e37fc5-d3f6-40f2-f863-7e4b2f89a0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [11:02<00:00, 15.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 0.63\n",
            "F1 Score: 0.74\n",
            "ROUGE-1: 0.75\n",
            "ROUGE-2: 0.49\n",
            "ROUGE-L: 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhsN4w26uKrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}